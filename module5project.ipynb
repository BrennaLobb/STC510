{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "module5project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrennaLobb/STC510/blob/main/module5project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyK7-J7Bdikm"
      },
      "source": [
        "#Parse, clean, and organize the Jeopardy! question data file to train a Naive Bayesian classifier."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUI37ZlAdqnX"
      },
      "source": [
        "import os\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import download\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "from datetime import datetime\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from re import sub"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNFnMCj9exkR"
      },
      "source": [
        "jeopardy_og_file = pd.read_json(\"jeopardy.json\")\n",
        "#jeopardy_og_file[:50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQSnwHDc9dr7"
      },
      "source": [
        "q_dict_value = jeopardy_og_file[['question','value']]\n",
        "q_list = list(jeopardy_og_file['question'])\n",
        "\n",
        "##this above line takes the questions column and money value and assigns it to it's own dictionary"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLLkRHjRIfax",
        "outputId": "bbb6e65f-bbde-4210-a36f-3c5877a52913"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miLdHjr7LD-4"
      },
      "source": [
        "# below gives us the sentences tokenized and results in a list of lists (1 list per each sentence)\n",
        "sent_list = []\n",
        "for x in q_list:\n",
        "  sent_list.append(sent_tokenize(x.lower()))"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xC-tDo7MPVe"
      },
      "source": [
        "English_stopwords = (stopwords.words('english') + list(punctuation))\n",
        "stop_sent_list = []\n",
        "for x in sent_list:\n",
        "  if not x in English_stopwords:\n",
        "    stop_sent_list.append(x)\n",
        "#[x for x in sent_list if not x in English_stopwords]"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WPs_7caQ5St"
      },
      "source": [
        "#NEED TO STEM EACH QUESTION, NOT THE WORDS!!!\n",
        "#now for stemming the sentences\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_qlist = []\n",
        "stem_q = []\n",
        "for x in stop_sent_list:\n",
        "  stemmed_qlist.append(stemmer.stem(str(x)))"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pai-SgOuXlGA"
      },
      "source": [
        "combo_dict = pd.DataFrame([value_list, stemmed_qlist])\n",
        "df = combo_dict.T\n",
        "df.columns = ['money_value','question']\n",
        "#combo dict now has the money values (as keys) assigned to their cleaned/parsed questions\n",
        "#need to transpose the dataframe, and assign column names after"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0obyHOpw30U"
      },
      "source": [
        "# need to change money values into integers (no commas and no dollar signs)\n",
        "df['money_value'].loc[df['money_value'] == \"$200\"] = \"low\"\n",
        "df['money_value'].loc[df['money_value'] == \"$100\"] = \"low\"\n",
        "df['money_value'].loc[df['money_value'] == \"$300\"] = \"low\"\n",
        "df['money_value'].loc[df['money_value'] == \"$400\"] = \"low\"\n",
        "df['money_value'].loc[df['money_value'] == \"$500\"] = \"low\"\n",
        "df['money_value'].loc[df['money_value'] == \"$600\"] = \"low\"\n",
        "df['money_value'].loc[df['money_value'] == \"$700\"] = \"low\"\n",
        "df['money_value'].loc[df['money_value'] == \"$800\"] = \"low\"\n",
        "df['money_value'].loc[df['money_value'] == \"$1000\"] = \"high\"\n",
        "df['money_value'].loc[df['money_value'] == \"$1200\"] = \"high\"\n",
        "df['money_value'].loc[df['money_value'] == \"$1400\"] = \"high\"\n",
        "df['money_value'].loc[df['money_value'] == \"$1600\"] = \"high\"\n",
        "df['money_value'].loc[df['money_value'] == \"$1800\"] = \"high\"\n",
        "df['money_value'].loc[df['money_value'] == \"$2000\"] = \"high\""
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyNXRFgm9kfQ"
      },
      "source": [
        "df = df.dropna()"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4DNlHT7T2Qk"
      },
      "source": [
        "# create a binary classifier (\"high value\" and \"low value,\" based on the points available for each) for questions\n",
        "#\"high value\" is for high cost questions, more money to do it right\n",
        "#\"low value\" is for cheaper questions\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.question, \n",
        "                                                    df.money_value, random_state=1)\n",
        "Tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
        "X_train_tf = Tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tf = Tfidf_vectorizer.transform(X_test)"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aShxAV5zrxcG",
        "outputId": "9349b265-c1c4-446b-bc24-22b90b0c31b2"
      },
      "source": [
        "#now to run them through naive Bayes:\n",
        "naive_bayes = MultinomialNB()\n",
        "naive_bayes.fit(X_train_tf, y_train)\n",
        "predictions = naive_bayes.predict(X_test_tf)\n",
        "print(\"accuracy: \",accuracy_score(y_test,predictions))"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy:  0.7129435151151452\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}